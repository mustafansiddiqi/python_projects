#Linear Regressions and ML
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import warnings
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

warnings.filterwarnings("ignore")

# Load dataset
df = pd.read_csv("boston (1).csv")

# Display dataset information
df.info()
df.describe()

# Check for missing values
df.isna().sum()

# Calculate correlation matrix
correlation_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
plt.title('Correlation Matrix')
plt.show()

# Scatter plots for highly correlated features
plt.figure(figsize=(20, 5))
features = ['LSTAT', 'RM', 'PTRATIO', 'INDUS']
target = df['MEDV']
for i, col in enumerate(features):
    plt.subplot(1, len(features), i+1)
    plt.scatter(df[col], target, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('MEDV')
plt.show()

# Selecting features with high correlation for linear regression
X = df[['RM', 'LSTAT']]
y = df['MEDV']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred)}")
print(f"R-squared: {r2_score(y_test, y_pred)}")

# Boxplots to visualize outliers
plt.figure(figsize=(15, 10))
for i, column in enumerate(df.columns):
    plt.subplot(4, 4, i + 1)
    sns.boxplot(y=df[column])
    plt.title(column)
    plt.tight_layout()
plt.show()

# Variance Inflation Factor (VIF) for multicollinearity check
numeric_columns = df.select_dtypes(include=[np.number]).columns.drop('MEDV')
df_numeric_with_constant = add_constant(df[numeric_columns])
vif_data = pd.DataFrame()
vif_data['Feature'] = df_numeric_with_constant.columns
vif_data['VIF'] = [variance_inflation_factor(df_numeric_with_constant.values, i) for i in range(df_numeric_with_constant.shape[1])]
print(vif_data)

# Create a new feature to reduce multicollinearity
df['RADxTAX'] = df['RAD'] * df['TAX']

# Normalize and standardize data
min_max_scaler = MinMaxScaler()
std_scaler = StandardScaler()
numeric_columns = df.select_dtypes(include=[np.number]).columns
columns_to_normalize = [col for col in numeric_columns if col != 'RM']
df[columns_to_normalize] = min_max_scaler.fit_transform(df[columns_to_normalize])
df['RM'] = std_scaler.fit_transform(df[['RM']])

# Train regression models with and without high multicollinearity features
def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return mean_squared_error(y_test, y_pred)

# Exclude high multicollinearity features
df_reduced = df.drop(columns=['RAD', 'RADxTAX', 'TAX'])
df_reduced = df_reduced.select_dtypes(include=[np.number])
X = df_reduced.drop('MEDV', axis=1)
y = df_reduced['MEDV']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train different models
lr_model = LinearRegression()
ridge_model = Ridge(alpha=1.0)
lasso_model = Lasso(alpha=0.1)

mse_lr = train_and_evaluate_model(lr_model, X_train, y_train, X_test, y_test)
mse_ridge = train_and_evaluate_model(ridge_model, X_train, y_train, X_test, y_test)
mse_lasso = train_and_evaluate_model(lasso_model, X_train, y_train, X_test, y_test)

print("Linear Regression MSE:", mse_lr)
print("Ridge Regression MSE:", mse_ridge)
print("Lasso Regression MSE:", mse_lasso)

# Model including high multicollinearity features
df_full = df.select_dtypes(include=[np.number])
X = df_full.drop('MEDV', axis=1)
y = df_full['MEDV']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

mse_lr_full = train_and_evaluate_model(lr_model, X_train, y_train, X_test, y_test)
mse_ridge_full = train_and_evaluate_model(ridge_model, X_train, y_train, X_test, y_test)
mse_lasso_full = train_and_evaluate_model(lasso_model, X_train, y_train, X_test, y_test)

print("Linear Regression MSE (Full):", mse_lr_full)
print("Ridge Regression MSE (Full):", mse_ridge_full)
print("Lasso Regression MSE (Full):", mse_lasso_full)
